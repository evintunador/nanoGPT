# Multi-run configuration for hyperparameter sweeps
# This example shows how to run multiple training runs with different learning rates and weight decay values

name: "nano_gpt_lr_wd_sweep"


# Define the command structure
command:
  type: python  # Options: python, torchrun, sbatch
  script: train.py
  # nproc_per_node: 1  # Number of GPUs per node for torchrun
  # Optional torchrun parameters:
  # nnodes: 1
  # node_rank: 0
  # master_addr: localhost
  # master_port: 29500


# Define parameters to pass to the script
# Parameters with list values will be swept over (grid search)
# Parameters with single values will be constant across all runs
parameters:
  # Static parameters - same for all runs
  config: config/train_shakespeare_char.yaml
  device: mps
  max_iters: 500
  eval_interval: 100
  eval_iters: 20
  n_layer: 4
  n_head: 4
  n_embd: 128
  block_size: 64
  batch_size: 12
  
  # Sweep parameters - will generate all combinations
  learning_rate: [1.0e-4, 1.0e-3, 1.0e-2]
  weight_decay: [0.0, 0.1]


# Execution settings
execution:
  mode: sequential  # Options: sequential, parallel
  max_workers: 1  # Only used when mode is parallel