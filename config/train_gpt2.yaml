# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py --config config/train_gpt2.yaml

out_dir: 'out'
eval_interval: 1000
log_interval: 10
eval_iters: 200
eval_only: false
always_save_checkpoint: true

wandb_log: true
wandb_project: 'owt'
wandb_run_name: 'gpt2-124M'

# data
dataset: 'openwebtext'
# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size: 12
block_size: 1024
gradient_accumulation_steps: 40 # 5 * 8

# model
n_layer: 12
n_head: 12
n_embd: 768
dropout: 0.0
bias: false

# adamw optimizer
learning_rate: 6.0e-4
# this makes total number of tokens be 300B
max_iters: 600000
weight_decay: 1.0e-1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

# learning rate decay settings
decay_lr: true
warmup_iters: 2000
lr_decay_iters: 600000
min_lr: 6.0e-5

# DDP settings
backend: 'nccl'

# system
device: 'cuda'
dtype: 'bfloat16'
compile: true
init_from: 'scratch'

